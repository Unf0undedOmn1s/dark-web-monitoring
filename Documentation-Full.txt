Dark-Web Monitoring (main.py)
Breakdown of main.py:

1. Imports Required Libraries:
- requests: Sends HTTP requests to the .onion sites through the Tor network.
- BeautifulSoupParses HTML to extract relevant titles and meta descriptions.
- datetime: Logs the timestamp of each scan.
- database.db_manager: Handles SQLite database operations (creating tables, inserting and retreiving data).

2. Defining the List of Target Dark-Web URLs.
Purpose: These .onion links are hidden services on the dark web that the script will scan.
Matter: .onion domains are not indexed by standard search engines like Google, or Firefox, requiring Tor routing.

3. Defining Keywords for Threat Detection.
Purpose: The script will search for the words inserted in the script in the website's title and description.
Matter: Helps in automatically detecting potentially malicious sites without manual intervation.

4. Function to scrape Dark-Web Pages scrape_url(url).
Purpose: Uses requests.get() to send a GET request to each .onion site.
Uses a Tor SOCKS5 proxy (127.0.0.1:9050) to route traffic through the Tor network.
Calls raise_for_status() to handle HTTP errors.

5. Extracting Webpage Data.
Purpose: Parses HTML content of the page.
Extracts the title (<title> tag).
Extracts the meta descrption (<meta name = "description"> tag).

Why use BeautifulSoup?
Dark web sites may lack proper structure, so BeautifulSoup helps extract meaningful content.

6. Checking for Suspicious Keywords.
Purpose: Converts both title and description to lowercase.
Searches for matching keywords from KEYWORDS list.
Matter: Detects blackhat activities without manually reviewing each site.

7. Storing Data in the SQLite Database.
Purpose: Converts the current timestamp into a string.
Calls insert_data() to store the extracted information in an SQLite database.

8. Handling Connection Errors.
Purpose: Catches connection issues, timeouts, or Tor misconfigurations.
Matter: Dark web sites are unstable and often go offline, so error handling is critical.

9. Running the Scan.
Purpose: Ensures the database exists (create_database()).
Iterates through all .onion URLs.
Calls scrape_url(url) to scan each site.
Matter: Allows batch processing of multiple .onion links.

10. Running the Script.
Purpose: Ensures main.py only runs when executed directly.
Calls run_scan() to start scanning.
Calls display_results() to show past results.
Matter (if __name__ == "__main__":): Prevents automatic execution when main.py is imported into another script.
---------------------------------------------------------------------------------------------------------------
Breakdown of db_manager.py (Dark-Web Monitoring/database/db_manager.py)

1. Importing Required Libraries.
- sqlite3: Allows interaction with a lightweight, file-based SQLite database.
- os: Helps manage file paths dynamically, ensuring compatibility across different operating systems.

2. Defining Database Constants.
Purpose: Defines the database file name.
Uses os.path.join() to construct a platform-independent path to the database.
Matter (os.path.join()): Ensures the database is always located in the same directory as db_manager.py.
Works on both Windows and Linux.

3. Creating Database (If it does not exist).
Purpose: Checks if the database file already exists.
If not found, it creates a new database and calls create_table().
Matter: Prevents overwriting an existing database each time the script runs.

4. Creating the results Table.
Purpose: Creates a table named results with five columns:
a) id → Auto-incremented primary key.
b) title → Stores the title of the webpage.
c) description → Stores the meta description.
d) keywords → Stores matched keywords.
e) timestamp → Stores the date and time of the scan.
Matter (AUTOINCREMENT for id): Ensures each record gets a unique identifier, making data retrieval easier.

5. Inserting Data into the Database.
Purpose: Inserts a new row into the results table.
Uses parameterized queries (?) to prevent SQL injection.
Why Use ? Instead of Concatenation: Prevents SQL injection attacks.
Ensures safe handling of user input.

6. Retrieving Data from the Database.
Purpose: Fetches all stored records from the database.
Prints them in a readable format.
Why Use fetchall(): Retrieves all rows at once, making it easier to process the results.
---------------------------------------------------------------------------------------------------------------
Breakdown of scraper.py (Dark-Web Monitoring/crawler/scraper.py)

1. Importing Required Libraries.
- requests: Fetches .onion pages over the Tor Network
- BeautifulSoup: Parses HTML to extract relevant titles and meta descriptions.
- store_result (from alerts.py): Saves the extracted data for further processing.

2. Fetching Onion Pages via Tor.
Uses Tor's SOCKS5 proxy (127.0.0.1:9050) to fetch hidden services (.onion sites).
Sets a timeout of 10 seconds to prevent hanging requests.
Handles errors, printing a message if fetching fails.
Why Use a Proxy: .onion websites cannot be accessed directly via a regular internet connection.
Tor anonymizes requests, making them untraceable.

3. Extracting Titles from the Page.
What it does: Parses HTML content using BeautifulSoup.
Finds all <h1> elements and extracts their text.
Why Extract Titles: Main headings (<h1>) summarize the page's purpose.
Useful for categorizing and indexing dark web content.

4. Extracting Links from the Page.
What It Does: Finds all <a> tags (hyperlinks) in the page.
Filters only valid HTTP/HTTPS links (ignores relative paths).
Why Extract Links: Maps connections between dark web sites.
Finds related hidden services.

5. Scraping and Storing Results.
What It Does: Fetches the .onion page.
Extracts titles and links.
Stores them using store_result() from alerts.py.
Why Store the Results: Helps in dark web monitoring.
Saves valuable OSINT (Open Source Intelligence) data.
---------------------------------------------------------------------------------------------------------------
Breakdown of alerts.py (Dark-Web Monitoring/alerts/alerts.py)
1. Importing Required Libraries.
- logging: Used to send alerts when suspicious words were found.
- sqlite3: Connects toa SQLite database to store and retrieve data.

2. Storing Results in the Database.
What It does: Connects to the darkweb_results.db SQLite database.
Inserts a new title, description and keyword into the alerts table.
Saves the data and closes the connection.
Matter: Keeping a record of suspicious content for further investigation.

3. Scanning for Keywords and Storing Matches.
What It Does: Iterates through stored data (data parameter).
Checks if any keyword is present in the title or description.
If a match is found, it prints the keyword and stores the result in the database.
Matter: Tracking specific dark web abilities and store alerts for further review.

4. Scanning for Keywords and Sending Notifications.
What It Does: Similar to scan_keywords_and_store(), but instead, it logs a notification.
Matter: To alert administrators when a keyword is found, allowing them to take immediate action.
---------------------------------------------------------------------------------------------------------------
Breakdown of logger.py (Dark-Web Monitoring/alerts/logger.py)
1. Importing Required Libraries.
- logging:mhandles event logging.

2. Configuring the Logger:
What It Does: Saves logs to a file called alerts.log.
Logs everything at level INFO of higher (INFO, WARNING, ERROR).
Adds timestamps to each log entry.
Matter: Recording activity to debug issues.

3. Creating the Logger Instance.
What It Does: Initializes a logger instance that will be used in other scripts.

4. Logging Events.
What It Does: Logs normal events at the INFO level.
Example Usage: log_event("Dark web scan completed successfully.")

5. Logging Errors.
What It Does: Logs errors at the ERROR level.
Example Usage: log_error("Failed to connect to the Tor proxy.")

Final Thoughts: This project provides a structured and automated approach to monitoring dark web activity.
By integrating web scraping, database storage, keyword detection, and logging, it enables analysts to efficiently track and respond to potential threats.
