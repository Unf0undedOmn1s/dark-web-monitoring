Dark-Web Monitoring (main.py)
Breakdown of main.py:

1. Imports Required Libraries:
- requests: Sends HTTP requests to the .onion sites through the Tor network.
- BeautifulSoupParses HTML to extract relevant titles and meta descriptions.
- datetime: Logs the timestamp of each scan.
- database.db_manager: Handles SQLite database operations (creating tables, inserting and retreiving data).

2. Defining the List of Target Dark-Web URLs.
Purpose: These .onion links are hidden services on the dark web that the script will scan.
Matter: .onion domains are not indexed by standard search engines like Google, or Firefox, requiring Tor routing.

3. Defining Keywords for Threat Detection.
Purpose: The script will search for the words inserted in the script in the website's title and description.
Matter: Helps in automatically detecting potentially malicious sites without manual intervation.

4. Function to scrape Dark-Web Pages scrape_url(url).
Purpose: Uses requests.get() to send a GET request to each .onion site.
Uses a Tor SOCKS5 proxy (127.0.0.1:9050) to route traffic through the Tor network.
Calls raise_for_status() to handle HTTP errors.

5. Extracting Webpage Data.
Purpose: Parses HTML content of the page.
Extracts the title (<title> tag).
Extracts the meta descrption (<meta name = "description"> tag).

Why use BeautifulSoup?
Dark web sites may lack proper structure, so BeautifulSoup helps extract meaningful content.

6. Checking for Suspicious Keywords.
Purpose: Converts both title and description to lowercase.
Searches for matching keywords from KEYWORDS list.
Matter: Detects blackhat activities without manually reviewing each site.

7. Storing Data in the SQLite Database.
Purpose: Converts the current timestamp into a string.
Calls insert_data() to store the extracted information in an SQLite database.

8. Handling Connection Errors.
Purpose: Catches connection issues, timeouts, or Tor misconfigurations.
Matter: Dark web sites are unstable and often go offline, so error handling is critical.

9. Running the Scan.
Purpose: Ensures the database exists (create_database()).
Iterates through all .onion URLs.
Calls scrape_url(url) to scan each site.
Matter: Allows batch processing of multiple .onion links.

10. Running the Script.
Purpose: Ensures main.py only runs when executed directly.
Calls run_scan() to start scanning.
Calls display_results() to show past results.
Matter (if __name__ == "__main__":): Prevents automatic execution when main.py is imported into another script.
---------------------------------------------------------------------------------------------------------------
Breakdown of db_manager.py (Dark-Web Monitoring/database/db_manager.py)

1. Importing Required Libraries.
- sqlite3: Allows interaction with a lightweight, file-based SQLite database.
- os: Helps manage file paths dynamically, ensuring compatibility across different operating systems.

2. Defining Database Constants.
Purpose: Defines the database file name.
Uses os.path.join() to construct a platform-independent path to the database.
Matter (os.path.join()): Ensures the database is always located in the same directory as db_manager.py.
Works on both Windows and Linux.

3. Creating Database (If it does not exist).
Purpose: Checks if the database file already exists.
If not found, it creates a new database and calls create_table().
Matter: Prevents overwriting an existing database each time the script runs.

4. Creating the results Table.
Purpose: Creates a table named results with five columns:
a) id → Auto-incremented primary key.
b) title → Stores the title of the webpage.
c) description → Stores the meta description.
d) keywords → Stores matched keywords.
e) timestamp → Stores the date and time of the scan.
Matter (AUTOINCREMENT for id): Ensures each record gets a unique identifier, making data retrieval easier.

5. Inserting Data into the Database.
Purpose: Inserts a new row into the results table.
Uses parameterized queries (?) to prevent SQL injection.
Why Use ? Instead of Concatenation: Prevents SQL injection attacks.
Ensures safe handling of user input.

6. Retrieving Data from the Database.
Purpose: Fetches all stored records from the database.
Prints them in a readable format.
Why Use fetchall(): Retrieves all rows at once, making it easier to process the results.
---------------------------------------------------------------------------------------------------------------
Breakdown of scraper.py (Dark-Web Monitoring/crawler/scraper.py)

1. 
